$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json
environment:
  python_requirements_txt: requirements.txt
inputs:
  user_input:
    type: string
  llm_endpoint:
    type: string
    default: http://127.0.0.1:8000/predict
  chat_history:
    type: list
outputs:
  chat_output:
    type: string
    reference: ${process_llm_output.output}
nodes:
- name: prompt
  type: prompt
  source:
    type: code
    path: components/code_prompt.jinja2
  inputs:
    user_input: ${inputs.user_input}
    chat_history: ${inputs.chat_history}
- name: call_local_llm_server
  type: python
  source:
    type: code
    path: ./components/call_local_llm_server.py
  inputs:
    max_tokens: 2048
    prompt: ${prompt.output}
    llm_endpoint: ${inputs.llm_endpoint}
- name: process_llm_output
  type: python
  source:
    type: code
    path: ./components/process_llm_output.py
  inputs:
    llm_output: ${call_local_llm_server.output}
